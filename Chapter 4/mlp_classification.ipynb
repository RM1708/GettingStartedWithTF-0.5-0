{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "mnist is of type base.Datasets. Datasets are namedtuples (See https://docs.python.org/3.6/library/collections.html#collections.namedtuple).\n",
    "\n",
    "Datasets are defined in /home/rm/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py. Datasets contain three objects of type mnist.DataSet.\n",
    "\n",
    "The class DataSet is defined in /home/rm/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py.\n",
    "\n",
    "The class Dataset has the following properties & methods that are used in the code below.\n",
    "\n",
    "    the property: num_examples the method: next_batch()\n",
    "\n",
    "For code that illustrates the above, see the set of print() statements in MNIST-mod1.ipynb\n",
    "## Change to the code\n",
    "\n",
    "mnist.Datasets are no longer to be used. The training, validation, and test data will now be read in differently. So the functionality provided by mnist.Datasets have to be replicated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the jupyter notebook that interfaces to the module that reads the MINST data\n",
    "#The read functionality is provided by Jupyter notebook: \n",
    "    #/home/rm/cjalmeida/tf_mnist/data.ipynb\n",
    "#The MINST data files are in:  /home/rm/cjalmeida/input\n",
    "\n",
    "#See https://gist.github.com/DCAL12/1a872bd63bedfb7b12612c8a7ec0f52e#file-notebook_importing-py\n",
    "from nbextensions import notebook_importing\n",
    "from modGetMNIST_Data_Labels import fnGetCompleteListOfTraining_Data_Labels\n",
    "from modGetMNIST_Data_Labels import fnGetCompleteListOfTest_Data_Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 40\n",
    "display_step = 1\n",
    "######################\n",
    "#RM\n",
    "H_IN_PIXELS = 28\n",
    "W_IN_PIXELS = 28\n",
    "INPUT_SHAPE = [None, H_IN_PIXELS * W_IN_PIXELS] #Shape of Input data\n",
    "OUTPUT_SHAPE = [None, 10] #Shape of Labels\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT for the graph\n",
    "#######################\n",
    "# See \"Wrapping all together -> Switch between train and test set using Initializable iterator\"\n",
    "# in Tensorflow-Dataset-Tutorial/dataset_tutorial.ipynb\n",
    "#\n",
    "# create a placeholder to dynamically switch between batch sizes\n",
    "\n",
    "# tf Graph input\n",
    "batch_size = tf.placeholder(tf.int64)\n",
    "\n",
    "x, y = tf.placeholder(tf.float64, shape=INPUT_SHAPE), \\\n",
    "                tf.placeholder(tf.int8, shape=OUTPUT_SHAPE)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size).repeat()\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "BatchOfFeatures_Labels = iter.get_next()\n",
    "\n",
    "####################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer num features\n",
    "n_hidden_2 = 256 # 2nd layer num features\n",
    "n_input = INPUT_SHAPE[1] #784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = OUTPUT_SHAPE[1] #10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "Features = tf.placeholder(tf.float64, shape=INPUT_SHAPE)\n",
    "TrueLabels = tf.placeholder(tf.float64, shape=OUTPUT_SHAPE)\n",
    "\n",
    "#weights layer 1\n",
    "h = tf.Variable(tf.random_normal([n_input, n_hidden_1], dtype=tf.float64))\n",
    "#bias layer 1\n",
    "bias_layer_1 = tf.Variable(tf.random_normal([n_hidden_1], dtype=tf.float64))\n",
    "#layer 1\n",
    "layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(Features,h),bias_layer_1))\n",
    "\n",
    "#weights layer 2\n",
    "w = tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], dtype=tf.float64))\n",
    "#bias layer 2\n",
    "bias_layer_2 = tf.Variable(tf.random_normal([n_hidden_2], dtype=tf.float64))\n",
    "#layer 2\n",
    "layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1,w),bias_layer_2))\n",
    "\n",
    "#weights output layer\n",
    "output_weights = tf.Variable(tf.random_normal([n_hidden_2, n_classes], dtype=tf.float64))\n",
    "#biar output layer\n",
    "bias_output = tf.Variable(tf.random_normal([n_classes], dtype=tf.float64))\n",
    "#output layer\n",
    "output_layer = tf.matmul(layer_2, output_weights) + bias_output\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output_layer, labels=TrueLabels))\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) \n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "  \n",
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(output_layer, 1), tf.argmax(TrueLabels, 1))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n",
    "\n",
    "# Initializing the variables\n",
    "init_vars = tf.global_variables_initializer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot settings\n",
    "avg_set = []\n",
    "epoch_set=[]\n",
    "\n",
    "#Read the complete MNIST training data file\n",
    "training_images, training_labels, no_of_training_images = fnGetCompleteListOfTraining_Data_Labels()\n",
    "\n",
    "#print(\"training_images.shape, before flattening: \", training_images.shape)\n",
    "assert(60000 == no_of_training_images)\n",
    "assert(training_images.shape == (no_of_training_images, H_IN_PIXELS, W_IN_PIXELS, 1))\n",
    "training_images = training_images.reshape(no_of_training_images, 28 * 28)\n",
    "#print(\"training_images.shape after flattening: \", training_images.shape)\n",
    "assert(training_images.shape == (no_of_training_images, H_IN_PIXELS * W_IN_PIXELS))\n",
    "assert(training_labels.shape == (no_of_training_images, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_vars)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_no_of_batches = no_of_training_images//BATCH_SIZE\n",
    "######################################\n",
    "        # initialise iterator with train data\n",
    "        sess.run(iter.initializer, feed_dict={ x: training_images,\\\n",
    "                                              y: training_labels, \\\n",
    "                                              batch_size: BATCH_SIZE})\n",
    "######################################\n",
    "        # Loop over all batches\n",
    "        for batch_no in range(total_no_of_batches):\n",
    "            #if(0 == batch_no):\n",
    "            #    print(\"Epoch No: {}, Batch No:{}\".format(epoch, batch_no))\n",
    "            ListOfFeatures_Labels = sess.run(BatchOfFeatures_Labels)\n",
    "            batch_Features = ListOfFeatures_Labels[0].astype(np.float64)\n",
    "            batch_Labels = ListOfFeatures_Labels[1]\n",
    "\n",
    "            assert(batch_Features.shape == (BATCH_SIZE, H_IN_PIXELS * W_IN_PIXELS))\n",
    "\n",
    "            # Fit training using batch data\n",
    "            sess.run(optimizer, \\\n",
    "                     feed_dict={Features: batch_Features, \\\n",
    "                               TrueLabels: batch_Labels})\n",
    "            # Compute average loss\n",
    "            avg_cost += (sess.run(cost, \\\n",
    "                                 feed_dict={Features: batch_Features, \\\n",
    "                                           TrueLabels: batch_Labels}) \\\n",
    "                                )/total_no_of_batches\n",
    "        # Display logs per epoch step\n",
    "        if epoch % (1) == 0:\n",
    "            print (\"Epoch:\", '%04d' % (epoch), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "        avg_set.append(avg_cost)\n",
    "        epoch_set.append(epoch+1)\n",
    "        \n",
    "    #Plot it\n",
    "    plt.plot(epoch_set,avg_set, 'o', label='Logistic Regression Training phase')\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print (\"\\nDONE: Training phase\")\n",
    "    ## Test model\n",
    "    #Check accuracy using test data\n",
    "    test_images, test_labels, no_of_test_images = fnGetCompleteListOfTest_Data_Labels()\n",
    "\n",
    "    assert(10000 == no_of_test_images)\n",
    "    assert(test_images.shape == (no_of_test_images, H_IN_PIXELS, W_IN_PIXELS, 1))\n",
    "\n",
    "    test_images = (test_images.reshape(no_of_test_images, 28 * 28)).astype(np.float64)\n",
    "\n",
    "    assert(test_images.shape == (no_of_test_images, H_IN_PIXELS * W_IN_PIXELS))\n",
    "    assert(test_labels.shape == (no_of_test_images, 10))\n",
    "\n",
    "    print (\"Model accuracy (with Test Data):\", accuracy.eval({Features: test_images, \\\n",
    "                                          TrueLabels: test_labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
